{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23a98861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder  \n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea13307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4112 problems\n",
      "Class distribution:\n",
      "problem_class\n",
      "hard      1941\n",
      "medium    1405\n",
      "easy       766\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/problems.csv')\n",
    "df = df.fillna('')\n",
    "\n",
    "print(f\"Loaded {len(df)} problems\")\n",
    "print(f\"Class distribution:\")\n",
    "print(df['problem_class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da714b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n",
      "Extracted 143 numerical features\n"
     ]
    }
   ],
   "source": [
    "def extract_features(row):\n",
    "    \"\"\"Extract comprehensive features from problem text\"\"\"\n",
    "    text = (row['description'] + ' ' + \n",
    "            row['input_description'] + ' ' + \n",
    "            row['output_description']).lower()\n",
    "    title_text = row['title'].lower()\n",
    "    \n",
    "    features = {}\n",
    "\n",
    "    features['total_length'] = len(text)\n",
    "    features['word_count'] = len(text.split())\n",
    "    features['desc_length'] = len(row['description'])\n",
    "    features['input_desc_length'] = len(row['input_description'])\n",
    "    features['output_desc_length'] = len(row['output_description'])\n",
    "    features['title_length'] = len(title_text)\n",
    "\n",
    "    total_len = features['total_length']\n",
    "    features['desc_ratio'] = features['desc_length'] / total_len if total_len > 0 else 0\n",
    "    features['input_ratio'] = features['input_desc_length'] / total_len if total_len > 0 else 0\n",
    "    features['output_ratio'] = features['output_desc_length'] / total_len if total_len > 0 else 0\n",
    "\n",
    "    words = text.split()\n",
    "    features['unique_words'] = len(set(words))\n",
    "    features['unique_ratio'] = len(set(words)) / len(words) if words else 0\n",
    "    features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0\n",
    "    features['max_word_length'] = max([len(w) for w in words]) if words else 0\n",
    "    features['min_word_length'] = min([len(w) for w in words]) if words else 0\n",
    "\n",
    "    algo_keywords = {\n",
    "        'graph': ['graph', 'tree', 'node', 'edge', 'vertex', 'path', 'cycle', 'dag', \n",
    "                  'directed', 'undirected', 'adjacency', 'connected', 'component', 'spanning'],\n",
    "        'dp': ['dynamic', 'dp', 'memoization', 'optimal substructure', 'overlapping', \n",
    "               'state', 'transition', 'recurrence', 'tabulation'],\n",
    "        'greedy': ['greedy', 'greedily', 'locally optimal'],\n",
    "        'binary_search': ['binary search', 'bisection', 'lower bound', 'upper bound', 'log n', 'logarithmic'],\n",
    "        'sorting': ['sort', 'sorted', 'sorting', 'order', 'arrange', 'quicksort', 'mergesort'],\n",
    "        'dfs_bfs': ['dfs', 'bfs', 'depth first', 'breadth first', 'traversal', 'explore', 'visit'],\n",
    "        'shortest_path': ['shortest path', 'dijkstra', 'bellman', 'floyd', 'shortest distance', 'minimum distance'],\n",
    "        'advanced_ds': ['segment tree', 'fenwick', 'trie', 'suffix array', 'union find', \n",
    "                        'disjoint set', 'sparse table', 'bit', 'binary indexed'],\n",
    "        'flow': ['max flow', 'min cut', 'network flow', 'bipartite matching', 'ford fulkerson'],\n",
    "        'string_algo': ['substring', 'palindrome', 'kmp', 'lcs', 'edit distance', 'pattern', \n",
    "                        'prefix', 'suffix', 'character', 'anagram', 'rabin karp'],\n",
    "        'number_theory': ['modulo', 'prime', 'gcd', 'lcm', 'factorial', 'coprime', 'euler', \n",
    "                          'divisor', 'multiple', 'sieve', 'totient'],\n",
    "        'combinatorics': ['permutation', 'combination', 'binomial', 'catalan', 'choose', 'arrangement'],\n",
    "        'probability': ['probability', 'expected value', 'random', 'expectation', 'distribution'],\n",
    "        'geometry': ['geometry', 'coordinate', 'polygon', 'convex hull', 'point', \n",
    "                     'line', 'angle', 'distance', 'euclidean', 'manhattan'],\n",
    "        'matrix': ['matrix', 'matrices', 'grid', 'board', '2d array'],\n",
    "        'two_pointer': ['two pointer', 'sliding window', 'subarray', 'contiguous', 'window'],\n",
    "        'bit_manipulation': ['bit', 'xor', 'and', 'or', 'bitwise', 'binary representation', 'shift'],\n",
    "        'backtracking': ['backtrack', 'generate all', 'all possible', 'enumerate', 'brute force', 'recursive'],\n",
    "        'divide_conquer': ['divide and conquer', 'divide', 'merge', 'split'],\n",
    "        'heap': ['heap', 'priority queue', 'heapify'],\n",
    "        'stack_queue': ['stack', 'queue', 'deque', 'fifo', 'lifo'],\n",
    "        'hashing': ['hash', 'hashmap', 'hashtable', 'dictionary', 'map'],\n",
    "        'simulation': ['simulate', 'simulation', 'process', 'step by step']\n",
    "    }\n",
    "    \n",
    "    for key, keywords in algo_keywords.items():\n",
    "        features[f'has_{key}'] = int(any(kw in text for kw in keywords))\n",
    "        features[f'{key}_count'] = sum(text.count(kw) for kw in keywords)\n",
    "\n",
    "    title_keywords = {\n",
    "        'easy_title': ['simple', 'basic', 'easy', 'count', 'sum', 'find'],\n",
    "        'medium_title': ['find', 'calculate', 'compute', 'determine'],\n",
    "        'hard_title': ['maximum', 'minimum', 'optimal', 'complex', 'advanced']\n",
    "    }\n",
    "    \n",
    "    for key, keywords in title_keywords.items():\n",
    "        features[key] = int(any(kw in title_text for kw in keywords))\n",
    "    \n",
    "    features['has_optimization'] = int(any(w in text for w in \n",
    "        ['minimum', 'maximum', 'minimize', 'maximize', 'optimal', 'best', \n",
    "         'smallest', 'largest', 'least', 'most', 'shortest', 'longest']))\n",
    "    \n",
    "    features['count_minimum'] = text.count('minimum') + text.count('minimize')\n",
    "    features['count_maximum'] = text.count('maximum') + text.count('maximize')\n",
    "    features['count_optimal'] = text.count('optimal')\n",
    "    features['count_shortest'] = text.count('shortest')\n",
    "    features['count_longest'] = text.count('longest')\n",
    "   \n",
    "    numbers = re.findall(r'\\d+', text)\n",
    "    constraint_keys = ['max_constraint', 'min_constraint', 'avg_constraint', 'median_constraint',\n",
    "                      'std_constraint', 'log_max_constraint', 'log_min_constraint',\n",
    "                      'num_constraints', 'constraint_range', 'tiny_constraint',\n",
    "                      'very_small_constraint', 'small_constraint', 'medium_small_constraint',\n",
    "                      'medium_constraint', 'medium_large_constraint', 'large_constraint',\n",
    "                      'very_large_constraint', 'huge_constraint', 'estimated_complexity',\n",
    "                      'constraint_variance', 'has_large_and_small', 'num_large_constraints',\n",
    "                      'num_small_constraints']\n",
    "    \n",
    "    if numbers:\n",
    "        nums = [int(n) for n in numbers if len(n) <= 10]\n",
    "        if nums:\n",
    "            max_num = max(nums)\n",
    "            min_num = min(nums)\n",
    "            features['max_constraint'] = min(max_num, 1e9)\n",
    "            features['min_constraint'] = min_num\n",
    "            features['avg_constraint'] = np.mean(nums)\n",
    "            features['median_constraint'] = np.median(nums)\n",
    "            features['std_constraint'] = np.std(nums)\n",
    "            features['log_max_constraint'] = np.log10(max_num + 1)\n",
    "            features['log_min_constraint'] = np.log10(min_num + 1)\n",
    "            features['num_constraints'] = len(nums)\n",
    "            features['constraint_range'] = max_num - min_num\n",
    "            \n",
    "            features['tiny_constraint'] = int(max_num <= 10)\n",
    "            features['very_small_constraint'] = int(10 < max_num <= 20)\n",
    "            features['small_constraint'] = int(20 < max_num <= 100)\n",
    "            features['medium_small_constraint'] = int(100 < max_num <= 1000)\n",
    "            features['medium_constraint'] = int(1000 < max_num <= 10000)\n",
    "            features['medium_large_constraint'] = int(10000 < max_num <= 100000)\n",
    "            features['large_constraint'] = int(100000 < max_num <= 1000000)\n",
    "            features['very_large_constraint'] = int(1000000 < max_num <= 10000000)\n",
    "            features['huge_constraint'] = int(max_num > 10000000)\n",
    "\n",
    "            if max_num <= 20:\n",
    "                features['estimated_complexity'] = 1\n",
    "            elif max_num <= 100:\n",
    "                features['estimated_complexity'] = 2\n",
    "            elif max_num <= 1000:\n",
    "                features['estimated_complexity'] = 3\n",
    "            elif max_num <= 100000:\n",
    "                features['estimated_complexity'] = 4\n",
    "            else:\n",
    "                features['estimated_complexity'] = 5\n",
    "            \n",
    "            features['constraint_variance'] = np.var(nums)\n",
    "            features['has_large_and_small'] = int(max_num > 10000 and min_num < 100)\n",
    "            features['num_large_constraints'] = sum(1 for n in nums if n > 10000)\n",
    "            features['num_small_constraints'] = sum(1 for n in nums if n < 100)\n",
    "        else:\n",
    "            for key in constraint_keys:\n",
    "                features[key] = 0\n",
    "    else:\n",
    "        for key in constraint_keys:\n",
    "            features[key] = 0\n",
    "\n",
    "    features['modulo_count'] = text.count('modulo') + text.count(' mod ') + text.count('10^9+7')\n",
    "    features['has_queries'] = int('quer' in text)\n",
    "    features['query_count'] = text.count('query') + text.count('queries')\n",
    "    features['testcase_count'] = text.count('test case') + text.count('testcase')\n",
    "    features['formula_count'] = text.count('$')\n",
    "    features['code_count'] = text.count('```') + text.count('code')\n",
    "\n",
    "    sentences = [s for s in text.split('.') if s.strip()]\n",
    "    features['num_sentences'] = len(sentences)\n",
    "    features['avg_sentence_length'] = np.mean([len(s.split()) for s in sentences]) if sentences else 0\n",
    "    features['max_sentence_length'] = max([len(s.split()) for s in sentences]) if sentences else 0\n",
    "\n",
    "    if sentences:\n",
    "        features['max_words_per_sentence'] = max(len(s.split()) for s in sentences)\n",
    "        features['has_long_sentence'] = int(features['max_words_per_sentence'] > 30)\n",
    "    else:\n",
    "        features['max_words_per_sentence'] = 0\n",
    "        features['has_long_sentence'] = 0\n",
    "\n",
    "    features['has_nested_loop'] = int('nested' in text or 'n^2' in text or 'n 2' in text or 'n squared' in text)\n",
    "    features['has_linear'] = int('linear' in text or 'o n' in text or 'single pass' in text)\n",
    "    features['has_log'] = int('log' in text or 'logarithm' in text)\n",
    "    features['has_exponential'] = int('exponential' in text or '2^n' in text or 'n!' in text)\n",
    "\n",
    "    features['is_counting'] = int(any(w in text for w in ['how many', 'count', 'number of']))\n",
    "    features['is_existence'] = int(any(w in text for w in ['is there', 'exists', 'possible', 'can you']))\n",
    "    features['is_construction'] = int(any(w in text for w in ['construct', 'build', 'create', 'generate']))\n",
    "    features['is_optimization'] = int(any(w in text for w in ['minimum', 'maximum', 'optimal', 'best']))\n",
    "\n",
    "    features['is_interactive'] = int('interactive' in text or 'jury' in text)\n",
    "    features['is_decision'] = int(('yes' in text and 'no' in text) or \n",
    "                                   'possible' in text or 'impossible' in text)\n",
    "    \n",
    "    features['has_multiple_queries'] = int(features['query_count'] > 1)\n",
    "    features['has_updates'] = int('update' in text)\n",
    "    features['has_range'] = int('range' in text)\n",
    "    features['multi_test_cases'] = int('test case' in text and ('t' in text or 'multiple' in text))\n",
    "\n",
    "    features['has_2d_constraint'] = int(bool(re.search(r'(\\d+)\\s*[xX×]\\s*(\\d+)', text)))\n",
    "    features['has_multiple_arrays'] = text.count('array') + text.count('list')\n",
    "    features['has_matrix_ops'] = int('transpose' in text or 'rotate' in text or 'flip' in text)\n",
    "    features['has_special_output'] = int(any(w in text for w in \n",
    "        ['modulo', 'lexicographically', 'any valid', 'any order']))\n",
    "\n",
    "    features['is_dp_keywords'] = int(any(w in text for w in \n",
    "        ['optimal', 'maximum', 'minimum', 'subproblem', 'overlapping']))\n",
    "    features['is_graph_keywords'] = int(any(w in text for w in \n",
    "        ['connected', 'path', 'distance', 'reachable', 'component']))\n",
    "    features['is_greedy_keywords'] = int(any(w in text for w in \n",
    "        ['local', 'choice', 'always', 'never']))\n",
    "\n",
    "    title_words = title_text.split()\n",
    "    features['title_has_number'] = int(bool(re.search(r'\\d', title_text)))\n",
    "    features['title_complexity'] = len([w for w in title_words if len(w) > 6])\n",
    "    features['title_has_hard_words'] = int(any(w in title_text for w in \n",
    "        ['maximum', 'minimum', 'optimal', 'shortest', 'longest']))\n",
    "    features['title_word_count'] = len(title_words)\n",
    "\n",
    "    features['has_pattern'] = int('pattern' in text or 'sequence' in text or 'repeat' in text)\n",
    "    features['has_boundary'] = int('boundary' in text or 'edge case' in text or 'corner case' in text)\n",
    "    features['has_formula'] = int('formula' in text or 'equation' in text)\n",
    "    features['has_array'] = int('array' in text)\n",
    "    features['has_sequence'] = int('sequence' in text)\n",
    "    features['mentions_complexity'] = int(any(w in text for w in \n",
    "        ['complexity', 'time limit', 'memory limit', 'efficient']))\n",
    "    \n",
    "    complexity_score = (\n",
    "        features['has_dp'] * 5 +\n",
    "        features['has_graph'] * 4 +\n",
    "        features['has_advanced_ds'] * 6 +\n",
    "        features['has_flow'] * 7 +\n",
    "        features['has_shortest_path'] * 4 +\n",
    "        features['has_number_theory'] * 3 +\n",
    "        features['huge_constraint'] * 5 +\n",
    "        features['has_string_algo'] * 3 +\n",
    "        features['has_backtracking'] * 5 +\n",
    "        features['has_bit_manipulation'] * 3 +\n",
    "        features['has_geometry'] * 4 +\n",
    "        features['has_divide_conquer'] * 3 +\n",
    "        features['is_optimization'] * 2 +\n",
    "        features['has_multiple_queries'] * 3 +\n",
    "        features['is_interactive'] * 5 +\n",
    "        (6 if features['estimated_complexity'] == 5 else 0)\n",
    "    )\n",
    "    features['algo_complexity_score'] = complexity_score\n",
    "\n",
    "    features['length_complexity_ratio'] = features['total_length'] / (features['algo_complexity_score'] + 1)\n",
    "    features['words_per_sentence'] = features['word_count'] / (features['num_sentences'] + 1)\n",
    "    features['unique_per_length'] = features['unique_words'] / (features['total_length'] + 1)\n",
    "\n",
    "    algo_density = (\n",
    "        features['has_dp'] + features['has_graph'] + \n",
    "        features['has_greedy'] + features['has_binary_search']\n",
    "    ) / (features['word_count'] / 100 + 1)\n",
    "    features['algo_keyword_density'] = algo_density\n",
    "\n",
    "    features['has_multiple_test'] = int(text.count('test') > 2)\n",
    "    features['has_output_format'] = int('output format' in text or 'print' in text)\n",
    "    features['has_input_format'] = int('input format' in text or 'read' in text)\n",
    "    features['has_constraints_section'] = int('constraint' in text or 'limit' in text)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Extracting features...\")\n",
    "feature_dicts = df.apply(extract_features, axis=1)\n",
    "numerical_features_df = pd.DataFrame(list(feature_dicts))\n",
    "\n",
    "print(f\"Extracted {len(numerical_features_df.columns)} numerical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c8dcb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing complete\n"
     ]
    }
   ],
   "source": [
    "df[\"full_text\"] = (\n",
    "    df[\"title\"] + \" \" + df[\"title\"] + \" \" +  \n",
    "    df[\"description\"] + \" \" +\n",
    "    df[\"input_description\"] + \" \" +\n",
    "    df[\"output_description\"]\n",
    ")\n",
    "df[\"full_text\"] = df[\"full_text\"].str.lower()\n",
    "df[\"full_text\"] = df[\"full_text\"].apply(lambda x: re.sub(r\"[^a-z0-9 ]\", \" \", x))\n",
    "df[\"full_text\"] = df[\"full_text\"].apply(lambda x: \" \".join(x.split()))\n",
    "\n",
    "print(\"Text preprocessing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b3a988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3289, Test size: 823\n"
     ]
    }
   ],
   "source": [
    "X_text = df[\"full_text\"]\n",
    "X_numerical = numerical_features_df\n",
    "y_class = df[\"problem_class\"]\n",
    "y_score = df[\"problem_score\"]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_class_encoded = label_encoder.fit_transform(y_class)\n",
    "\n",
    "X_text_train, X_text_test, X_num_train, X_num_test, y_class_train, y_class_test, y_class_train_encoded, y_class_test_encoded, y_score_train, y_score_test = train_test_split(\n",
    "    X_text, X_numerical, y_class, y_class_encoded, y_score,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_class\n",
    ")\n",
    "\n",
    "print(f\"Train size: {len(X_text_train)}, Test size: {len(X_text_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49e713ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF vectorizers...\n",
      "Transforming text features...\n",
      "Scaling numerical features...\n",
      "Combining all features...\n",
      "Final feature shape: (3289, 18143)\n",
      "\n",
      "================================================================================\n",
      "PART 1 COMPLETE - Ready for model training!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "custom_stop_words = [\n",
    "    'given', 'find', 'output', 'input', 'first', 'second', \n",
    "    'line', 'integer', 'number', 'single', 'example', 'contain',\n",
    "    'follow', 'next', 'note', 'sample'\n",
    "]\n",
    "\n",
    "print(\"Creating TF-IDF vectorizers...\")\n",
    "tfidf_word = TfidfVectorizer(\n",
    "    max_features=15000, \n",
    "    ngram_range=(1, 4),\n",
    "    min_df=2, \n",
    "    max_df=0.7, \n",
    "    stop_words=list(set(list(ENGLISH_STOP_WORDS) + custom_stop_words)),\n",
    "    sublinear_tf=True,\n",
    "    use_idf=True,\n",
    "    norm='l2',\n",
    "    analyzer='word'\n",
    ")\n",
    "\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    max_features=3000,\n",
    "    ngram_range=(3, 6), \n",
    "    analyzer='char',\n",
    "    sublinear_tf=True,\n",
    "    min_df=2 \n",
    ")\n",
    "\n",
    "print(\"Transforming text features...\")\n",
    "X_text_train_word = tfidf_word.fit_transform(X_text_train)\n",
    "X_text_test_word = tfidf_word.transform(X_text_test)\n",
    "\n",
    "X_text_train_char = tfidf_char.fit_transform(X_text_train)\n",
    "X_text_test_char = tfidf_char.transform(X_text_test)\n",
    "\n",
    "X_text_train_tfidf = hstack([X_text_train_word, X_text_train_char])\n",
    "X_text_test_tfidf = hstack([X_text_test_word, X_text_test_char])\n",
    "\n",
    "print(\"Scaling numerical features...\")\n",
    "scaler = StandardScaler()\n",
    "X_num_train_scaled = scaler.fit_transform(X_num_train)\n",
    "X_num_test_scaled = scaler.transform(X_num_test)\n",
    "\n",
    "X_num_train_weighted = X_num_train_scaled * 5.0 \n",
    "X_num_test_weighted = X_num_test_scaled * 5.0\n",
    "\n",
    "print(\"Combining all features...\")\n",
    "X_train_combined = hstack([X_text_train_tfidf, csr_matrix(X_num_train_weighted)])\n",
    "X_test_combined = hstack([X_text_test_tfidf, csr_matrix(X_num_test_weighted)])\n",
    "\n",
    "print(f\"Final feature shape: {X_train_combined.shape}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PART 1 COMPLETE - Ready for model training!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea351b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING MODEL TRAINING\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, RandomForestRegressor, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_absolute_error, mean_squared_error, f1_score\n",
    "from xgboost import XGBClassifier  \n",
    "import joblib\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STARTING MODEL TRAINING\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9306f1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train_combined type: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "X_train_combined shape: (3289, 18143)\n",
      "\n",
      "Training classification models sequentially...\n",
      "Dataset size: 3289 training samples\n",
      "================================================================================\n",
      "\n",
      "[1/4] Training Extra Trees...\n",
      "Extra Trees: Accuracy=0.5395, F1=0.4465 (Time: 136.9s)\n",
      "\n",
      "[2/4] Training Random Forest...\n",
      "Random Forest: Accuracy=0.5249, F1=0.4212 (Time: 154.1s)\n",
      "\n",
      "[3/4] Training XGBoost...\n",
      "XGBoost: Accuracy=0.5395, F1=0.4662 (Time: 1132.9s)\n",
      "\n",
      "[4/4] Training Logistic Regression...\n",
      "Logistic Regression: Accuracy=0.4520, F1=0.4356 (Time: 54.0s)\n",
      "\n",
      "[5/5] Creating Fast Ensemble...\n",
      "Fast Ensemble: Accuracy=0.5334, F1=0.4380 (Time: 0.5s)\n",
      "\n",
      "================================================================================\n",
      "CLASSIFICATION TRAINING COMPLETE\n",
      "================================================================================\n",
      "Best Model: Extra Trees\n",
      "Accuracy: 0.5395 (53.95%)\n",
      "Macro F1: 0.4465\n",
      "\n",
      "All Models Performance:\n",
      "  Extra Trees         : Accuracy=0.5395 (53.95%), F1=0.4465\n",
      "  XGBoost             : Accuracy=0.5395 (53.95%), F1=0.4662\n",
      "  Fast Ensemble       : Accuracy=0.5334 (53.34%), F1=0.4380\n",
      "  Random Forest       : Accuracy=0.5249 (52.49%), F1=0.4212\n",
      "  Logistic Regression : Accuracy=0.4520 (45.20%), F1=0.4356\n",
      "\n",
      "================================================================================\n",
      "CLASSIFICATION REPORT\n",
      "================================================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        easy       0.57      0.33      0.42       153\n",
      "        hard       0.54      0.89      0.68       389\n",
      "      medium       0.43      0.15      0.22       281\n",
      "\n",
      "    accuracy                           0.53       823\n",
      "   macro avg       0.52      0.46      0.44       823\n",
      "weighted avg       0.51      0.53      0.47       823\n",
      "\n",
      "\n",
      "CONFUSION MATRIX\n",
      "================================================================================\n",
      "               Pred_easy  Pred_hard  Pred_medium\n",
      "Actual_easy           50         76           27\n",
      "Actual_hard           13        347           29\n",
      "Actual_medium         24        215           42\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nX_train_combined type: {type(X_train_combined)}\")\n",
    "print(f\"X_train_combined shape: {X_train_combined.shape}\")\n",
    "\n",
    "print(\"\\nTraining classification models sequentially...\")\n",
    "print(f\"Dataset size: {X_train_combined.shape[0]} training samples\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "models = {}\n",
    "\n",
    "print(\"\\n[1/4] Training Extra Trees...\")\n",
    "start = time.time()\n",
    "et_clf = ExtraTreesClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=25,\n",
    "    min_samples_split=3,\n",
    "    min_samples_leaf=2,\n",
    "    max_features=0.3,\n",
    "    class_weight='balanced_subsample',\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    criterion='entropy'\n",
    ")\n",
    "et_clf.fit(X_train_combined, y_class_train)\n",
    "et_pred = et_clf.predict(X_test_combined)\n",
    "et_acc = accuracy_score(y_class_test, et_pred)\n",
    "et_f1 = f1_score(y_class_test, et_pred, average='macro')\n",
    "models['Extra Trees'] = (et_clf, et_acc, et_f1)\n",
    "elapsed = time.time() - start\n",
    "print(f\"Extra Trees: Accuracy={et_acc:.4f}, F1={et_f1:.4f} (Time: {elapsed:.1f}s)\")\n",
    "\n",
    "print(\"\\n[2/4] Training Random Forest...\")\n",
    "start = time.time()\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=25,\n",
    "    min_samples_split=3,\n",
    "    min_samples_leaf=2,\n",
    "    max_features=0.3,\n",
    "    class_weight='balanced_subsample',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    criterion='entropy',\n",
    "    max_samples=0.85\n",
    ")\n",
    "rf_clf.fit(X_train_combined, y_class_train)\n",
    "rf_pred = rf_clf.predict(X_test_combined)\n",
    "rf_acc = accuracy_score(y_class_test, rf_pred)\n",
    "rf_f1 = f1_score(y_class_test, rf_pred, average='macro')\n",
    "models['Random Forest'] = (rf_clf, rf_acc, rf_f1)\n",
    "elapsed = time.time() - start\n",
    "print(f\"Random Forest: Accuracy={rf_acc:.4f}, F1={rf_f1:.4f} (Time: {elapsed:.1f}s)\")\n",
    "\n",
    "print(\"\\n[3/4] Training XGBoost...\")\n",
    "start = time.time()\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=12,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.85,\n",
    "    colsample_bytree=0.85,\n",
    "    colsample_bylevel=0.85,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=3.0,\n",
    "    min_child_weight=2,\n",
    "    gamma=0.2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='mlogloss',\n",
    "    tree_method='hist',\n",
    "    scale_pos_weight=1.5,\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_clf.fit(X_train_combined, y_class_train_encoded, verbose=False)\n",
    "xgb_pred_encoded = xgb_clf.predict(X_test_combined)\n",
    "xgb_pred = label_encoder.inverse_transform(xgb_pred_encoded)\n",
    "xgb_acc = accuracy_score(y_class_test, xgb_pred)\n",
    "xgb_f1 = f1_score(y_class_test, xgb_pred, average='macro')\n",
    "models['XGBoost'] = (xgb_clf, xgb_acc, xgb_f1)\n",
    "elapsed = time.time() - start\n",
    "print(f\"XGBoost: Accuracy={xgb_acc:.4f}, F1={xgb_f1:.4f} (Time: {elapsed:.1f}s)\")\n",
    "\n",
    "print(\"\\n[4/4] Training Logistic Regression...\")\n",
    "start = time.time()\n",
    "lr_clf = LogisticRegression(\n",
    "    max_iter=2000,\n",
    "    C=2.0,\n",
    "    class_weight='balanced',\n",
    "    solver='saga',\n",
    "    penalty='l2',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    tol=1e-3,\n",
    "    verbose=0\n",
    ")\n",
    "lr_clf.fit(X_train_combined, y_class_train)\n",
    "lr_pred = lr_clf.predict(X_test_combined)\n",
    "lr_acc = accuracy_score(y_class_test, lr_pred)\n",
    "lr_f1 = f1_score(y_class_test, lr_pred, average='macro')\n",
    "models['Logistic Regression'] = (lr_clf, lr_acc, lr_f1)\n",
    "elapsed = time.time() - start\n",
    "print(f\"Logistic Regression: Accuracy={lr_acc:.4f}, F1={lr_f1:.4f} (Time: {elapsed:.1f}s)\")\n",
    "\n",
    "print(\"\\n[5/5] Creating Fast Ensemble...\")\n",
    "start = time.time()\n",
    "\n",
    "et_proba = et_clf.predict_proba(X_test_combined)\n",
    "rf_proba = rf_clf.predict_proba(X_test_combined)\n",
    "xgb_proba_encoded = xgb_clf.predict_proba(X_test_combined)\n",
    "\n",
    "label_mapping = {label_encoder.transform([cls])[0]: i for i, cls in enumerate(et_clf.classes_)}\n",
    "xgb_proba = np.zeros_like(et_proba)\n",
    "for enc_idx, orig_idx in label_mapping.items():\n",
    "    xgb_proba[:, orig_idx] = xgb_proba_encoded[:, enc_idx]\n",
    "\n",
    "ensemble_proba = (et_proba + rf_proba + xgb_proba) / 3\n",
    "y_class_pred = et_clf.classes_[np.argmax(ensemble_proba, axis=1)]\n",
    "\n",
    "ensemble_acc = accuracy_score(y_class_test, y_class_pred)\n",
    "ensemble_f1 = f1_score(y_class_test, y_class_pred, average='macro')\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Fast Ensemble: Accuracy={ensemble_acc:.4f}, F1={ensemble_f1:.4f} (Time: {elapsed:.1f}s)\")\n",
    "models['Fast Ensemble'] = (None, ensemble_acc, ensemble_f1)\n",
    "\n",
    "best_model_name = max(models.items(), key=lambda x: x[1][1])[0]\n",
    "best_clf, best_acc, best_f1 = models[best_model_name]\n",
    "\n",
    "if best_model_name == 'Fast Ensemble':\n",
    "    best_clf = xgb_clf\n",
    "    print(f\"\\n✓ Ensemble won! Saving XGBoost for deployment.\")\n",
    "\n",
    "ensemble_clf = best_clf\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_acc:.4f} ({best_acc*100:.2f}%)\")\n",
    "print(f\"Macro F1: {best_f1:.4f}\")\n",
    "\n",
    "print(\"\\nAll Models Performance:\")\n",
    "for name, (_, acc, f1) in sorted(models.items(), key=lambda x: x[1][1], reverse=True):\n",
    "    print(f\"  {name:20s}: Accuracy={acc:.4f} ({acc*100:.2f}%), F1={f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_class_test, y_class_pred))\n",
    "\n",
    "print(\"\\nCONFUSION MATRIX\")\n",
    "print(\"=\"*80)\n",
    "labels = sorted(df['problem_class'].unique())\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_class_test, y_class_pred, labels=labels),\n",
    "    index=[f\"Actual_{c}\" for c in labels],\n",
    "    columns=[f\"Pred_{c}\" for c in labels]\n",
    ")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8ca87fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING SCORE REGRESSOR\n",
      "================================================================================\n",
      "Training regressor...\n",
      "MAE: 1.7240\n",
      "RMSE: 2.0434\n",
      "Correlation: 0.4116\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SCORE REGRESSOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "regressor = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=30,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training regressor...\")\n",
    "regressor.fit(X_train_combined, y_score_train)\n",
    "y_score_pred = regressor.predict(X_test_combined)\n",
    "\n",
    "mae = mean_absolute_error(y_score_test, y_score_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_score_test, y_score_pred))\n",
    "correlation = np.corrcoef(y_score_test, y_score_pred)[0, 1] if np.std(y_score_test) > 0 else 0.0\n",
    "\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Correlation: {correlation:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5e4afba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING MODELS\n",
      "================================================================================\n",
      "All models saved successfully!\n",
      "\n",
      "================================================================================\n",
      "TRAINING COMPLETE!\n",
      "================================================================================\n",
      "Final Classification Accuracy: 53.95%\n",
      "Final Classification F1: 0.4465\n",
      "Regression MAE: 1.7240\n",
      "\n",
      "Models saved in '../models/' directory\n",
      "Files created:\n",
      "  - text_vectorizer_word.pkl\n",
      "  - text_vectorizer_char.pkl\n",
      "  - numerical_scaler.pkl\n",
      "  - label_encoder.pkl\n",
      "  - difficulty_classifier.pkl\n",
      "  - score_regressor.pkl\n",
      "  - metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "joblib.dump(tfidf_word, \"../models/text_vectorizer_word.pkl\")\n",
    "joblib.dump(tfidf_char, \"../models/text_vectorizer_char.pkl\")\n",
    "joblib.dump(scaler, \"../models/numerical_scaler.pkl\")\n",
    "joblib.dump(label_encoder, \"../models/label_encoder.pkl\")\n",
    "joblib.dump(ensemble_clf, \"../models/difficulty_classifier.pkl\")\n",
    "joblib.dump(regressor, \"../models/score_regressor.pkl\")\n",
    "\n",
    "metadata = {\n",
    "    \"classifier_type\": best_model_name,\n",
    "    \"classifier_accuracy\": float(best_acc),\n",
    "    \"classifier_f1\": float(best_f1),\n",
    "    \"regressor_mae\": float(mae),\n",
    "    \"regressor_rmse\": float(rmse),\n",
    "    \"numerical_features\": list(X_numerical.columns),\n",
    "    \"n_text_features\": X_text_train_tfidf.shape[1],\n",
    "    \"n_total_features\": X_train_combined.shape[1],\n",
    "    \"all_models_performance\": {name: {\"accuracy\": float(acc), \"f1\": float(f1)} \n",
    "                               for name, (_, acc, f1) in models.items()}\n",
    "}\n",
    "joblib.dump(metadata, \"../models/metadata.pkl\")\n",
    "\n",
    "print(\"All models saved successfully!\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final Classification Accuracy: {best_acc*100:.2f}%\")\n",
    "print(f\"Final Classification F1: {best_f1:.4f}\")\n",
    "print(f\"Regression MAE: {mae:.4f}\")\n",
    "print(\"\\nModels saved in '../models/' directory\")\n",
    "print(\"Files created:\")\n",
    "print(\"  - text_vectorizer_word.pkl\")\n",
    "print(\"  - text_vectorizer_char.pkl\")\n",
    "print(\"  - numerical_scaler.pkl\")\n",
    "print(\"  - label_encoder.pkl\")\n",
    "print(\"  - difficulty_classifier.pkl\")\n",
    "print(\"  - score_regressor.pkl\")\n",
    "print(\"  - metadata.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
